{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WuUDWhfxpM4",
        "outputId": "1b3e5554-300b-4c08-b70f-515ca540f6af"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3-bj3v2yjTw",
        "outputId": "86ac17e5-246a-4fad-cf97-6858636f079c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Create a token in Hugging Face and replace 'your_token' with the actual token"
      ],
      "metadata": {
        "id": "Zc-Rrf0N4iE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import os\n",
        "os.environ['HF_TOKEN'] = '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'"
      ],
      "metadata": {
        "id": "g9fsjPmmyphk"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "HF_TOKEN = os.getenv('HF_TOKEN')\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large\", use_auth_token = HF_TOKEN)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large\", use_auth_token=HF_TOKEN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xvn1Us7s19Qf",
        "outputId": "1e12814b-ff7e-4393-9f0f-948cfb016384"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:786: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:469: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = [\n",
        "    \"User: Hello, how are you?\",\n",
        "    \"Assistant: I'm good, thank you! How about you?\",\n",
        "    \"User: What is AI?\",\n",
        "    \"Assistant: AI refers to computer systems performing tasks requiring human intelligence.\",\n",
        "\n",
        "    \"User: How does machine learning work?\",\n",
        "    \"Assistant: Machine learning trains algorithms on data for predictions.\",\n",
        "    \"User: What is NLP?\",\n",
        "    \"Assistant: NLP analyzes and generates human language.\",\n",
        "    \"User: Can AI create art?\",\n",
        "    \"Assistant: Yes, AI generates art, music, and stories.\",\n",
        "\n",
        "    \"User: How does facial recognition work?\",\n",
        "    \"Assistant: Facial recognition analyzes facial patterns.\",\n",
        "\n",
        "    \"User: What's the difference between deep learning and machine learning?\",\n",
        "    \"Assistant: Deep learning uses neural networks.\",\n",
        "\n",
        "    \"User: Can AI aid medical diagnosis?\",\n",
        "    \"Assistant: Yes, AI analyzes medical images and data.\",\n",
        "    \"User: How does AI impact jobs?\",\n",
        "    \"Assistant: AI automates some jobs, creates new ones.\",\n",
        "\n",
        "    \"User: What's the future of autonomous vehicles?\",\n",
        "    \"Assistant: Autonomous vehicles improve safety and efficiency.\",\n",
        "\n",
        "    \"User: Can AI understand emotions?\",\n",
        "    \"Assistant: AI recognizes emotions through affective computing.\",\n",
        "\n",
        "    \"User: How does AI assist customer service?\",\n",
        "    \"Assistant: AI-powered chatbots provide 24/7 support.\",\n",
        "\n",
        "    \"User: What are AI ethics concerns?\",\n",
        "    \"Assistant: Bias, privacy, transparency.\",\n",
        "\n",
        "    \"User: Can AI help with climate change?\",\n",
        "    \"Assistant: AI optimizes energy, predicts weather.\",\n",
        "\n",
        "    \"User: What's AI's role in cybersecurity?\",\n",
        "    \"Assistant: AI detects threats, predicts attacks.\"\n",
        "]\n",
        "\n",
        "input_text = \" \".join(chat_history)\n",
        "tokens = tokenizer.encode(input_text, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "svdP2ptH3TTg"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "import re\n",
        "import random\n",
        "\n",
        "def generate_mcqs(model, tokenizer, tokens):\n",
        "    outputs = model.generate(tokens, max_length=150)\n",
        "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Parse and format MCQs from decoded_output\n",
        "    mcqs = []\n",
        "    questions = re.split(r'Question |Answer:', decoded_output)[1:]\n",
        "\n",
        "    for i in range(0, len(questions), 2):\n",
        "        question = questions[i]\n",
        "        answer = questions[i+1].strip()\n",
        "\n",
        "        # Generate options\n",
        "        options = [answer]\n",
        "        for _ in range(3):\n",
        "            option = random.choice(questions[(i+1)%len(questions)])\n",
        "            options.append(option.strip())\n",
        "\n",
        "            # Shuffle options\n",
        "        random.shuffle(options)\n",
        "\n",
        "        # Find correct answer index\n",
        "        correct_answer_index = options.index(answer)\n",
        "\n",
        "        # Format MCQ\n",
        "        mcq = {\n",
        "            \"question\": question,\n",
        "            \"options\": options,\n",
        "            \"correct_answer\": chr(65 + correct_answer_index)  # A, B, C, D\n",
        "        }\n",
        "\n",
        "        mcqs.append(mcq)\n",
        "\n",
        "    return mcqs\n",
        "\n",
        "# Load pre-trained T5 model and tokenizer\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
        "\n",
        "# Input tokens\n",
        "input_text = \"Generate 5 MCQs on AI\"\n",
        "tokens = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "# Generate MCQs\n",
        "mcqs = generate_mcqs(model, tokenizer, tokens)\n",
        "\n",
        "# Print MCQs\n",
        "for i, mcq in enumerate(mcqs):\n",
        "    print(f\"Question {i+1}: {mcq['question']}\")\n",
        "    for j, option in enumerate(mcq['options']):\n",
        "        print(f\"{chr(65 + j)}. {option}\")\n",
        "    print(f\"Correct Answer: {mcq['correct_answer']}\")\n",
        "    print()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "l31Gpc4b_jFj"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate article\n",
        "def generate_article(model, tokenizer, tokens):\n",
        "    outputs = model.generate(tokens, max_length=300)\n",
        "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return decoded_output\n",
        "\n",
        "article = generate_article(model, tokenizer, tokens)\n",
        "print(article)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQaMfrVo3dB3",
        "outputId": "8d52067f-57a5-4549-cc39-8d1157b7723e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 MCQs on AI. Generate 5 MCQs on AI. Generate 5 MCQs on AI. Generate 5 MCQs on AI. Generate 5 MCQs on AI. Generate 5 MCQs on AI. Generate 5 MCQs on AI. Generate 5 MCQs on AI. Generate 5 MCQs on AI. Generate 5 MCQs on AI. Generate 5 MCQs on AI. Generate 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "class ChatAnalyzer:\n",
        "    # ... other methods ...\n",
        "\n",
        "    def generate_mcqs(self, chat_history, decoded_output):  # Add decoded_output as argument\n",
        "        # ... other code ...\n",
        "\n",
        "        # Example using regular expressions to extract MCQs\n",
        "        # The parenthesis within options a, b, c, d needs to be escaped using a backslash `\\`\n",
        "        mcq_pattern = r\"(.*?)\\?\\n(a\\)\\s*(.*?)\\n(b\\)\\s*(.*?)\\n(c\\)\\s*(.*?)\\n(d\\)\\s*(.*?))\"\n",
        "        matches = re.findall(mcq_pattern, decoded_output) # Removed extra comma\n",
        "\n",
        "        mcqs = []  # Initialize mcqs here\n",
        "        for match in matches:\n",
        "            question = match[0]\n",
        "            options = [match[2], match[4], match[6], match[8]]\n",
        "            # Add logic to identify correct answer (if available)\n",
        "            mcqs.append({\"question\": question, \"options\": options})\n",
        "\n",
        "        return mcqs"
      ],
      "metadata": {
        "id": "Cp_V0__f3kot"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "class ChatAnalyzer:\n",
        "    def __init__(self, model_name):\n",
        "        HF_TOKEN = os.getenv('HF_TOKEN')\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=HF_TOKEN)\n",
        "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name, use_auth_token=HF_TOKEN)\n",
        "\n",
        "    def preprocess_chat(self, chat_history):\n",
        "        input_text = \" \".join(chat_history)\n",
        "        tokens = self.tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "        return tokens\n",
        "\n",
        "    def generate_mcqs(self, chat_history):\n",
        "        tokens = self.preprocess_chat(chat_history)\n",
        "        outputs = self.model.generate(tokens, max_length=150)\n",
        "        decoded_output = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        # Parse and format MCQs from decoded_output\n",
        "        mcqs = self._parse_mcqs(decoded_output)\n",
        "        return mcqs\n",
        "\n",
        "    def generate_article(self, chat_history):\n",
        "        tokens = self.preprocess_chat(chat_history)\n",
        "        outputs = self.model.generate(tokens, max_length=300)\n",
        "        decoded_output = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return decoded_output\n",
        "\n",
        "    def _parse_mcqs(self, text):\n",
        "        # Simplified example parser, you should implement logic to parse MCQs from the generated text\n",
        "        return [text]\n",
        "\n",
        "# Example usage\n",
        "chat_history = [\n",
        "    \"User: Hello, how are you?\",\n",
        "    \"Assistant: I'm good, thank you! How about you?\",\n",
        "    \"User: What is AI?\",\n",
        "    \"Assistant: AI refers to computer systems performing tasks requiring human intelligence.\",\n",
        "\n",
        "    \"User: How does machine learning work?\",\n",
        "    \"Assistant: Machine learning trains algorithms on data for predictions.\",\n",
        "    \"User: What is NLP?\",\n",
        "    \"Assistant: NLP analyzes and generates human language.\",\n",
        "    \"User: Can AI create art?\",\n",
        "    \"Assistant: Yes, AI generates art, music, and stories.\",\n",
        "\n",
        "    \"User: How does facial recognition work?\",\n",
        "    \"Assistant: Facial recognition analyzes facial patterns.\",\n",
        "\n",
        "    \"User: What's the difference between deep learning and machine learning?\",\n",
        "    \"Assistant: Deep learning uses neural networks.\",\n",
        "\n",
        "    \"User: Can AI aid medical diagnosis?\",\n",
        "    \"Assistant: Yes, AI analyzes medical images and data.\",\n",
        "    \"User: How does AI impact jobs?\",\n",
        "    \"Assistant: AI automates some jobs, creates new ones.\",\n",
        "\n",
        "    \"User: What's the future of autonomous vehicles?\",\n",
        "    \"Assistant: Autonomous vehicles improve safety and efficiency.\",\n",
        "\n",
        "    \"User: Can AI understand emotions?\",\n",
        "    \"Assistant: AI recognizes emotions through affective computing.\",\n",
        "\n",
        "    \"User: How does AI assist customer service?\",\n",
        "    \"Assistant: AI-powered chatbots provide 24/7 support.\",\n",
        "\n",
        "    \"User: What are AI ethics concerns?\",\n",
        "    \"Assistant: Bias, privacy, transparency.\",\n",
        "\n",
        "    \"User: Can AI help with climate change?\",\n",
        "    \"Assistant: AI optimizes energy, predicts weather.\",\n",
        "\n",
        "    \"User: What's AI's role in cybersecurity?\",\n",
        "    \"Assistant: AI detects threats, predicts attacks.\"\n",
        "\n",
        "]\n",
        "\n",
        "analyzer = ChatAnalyzer(\"facebook/bart-large\")\n",
        "mcqs = analyzer.generate_mcqs(chat_history)\n",
        "article = analyzer.generate_article(chat_history)\n",
        "\n",
        "print(\"MCQs:\", mcqs)\n",
        "print(\"Article:\", article)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mncCiAc63thN",
        "outputId": "4549c8d6-1272-43ea-a32e-d43195f963a7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MCQs: [\"User: Hello, how are you? Assistant: I'm good, thank you! How about you? User: What's the difference between deep learning and machine learning? assistant: Deep learning uses neural networks. User: How does AI assist customer service? AI-powered chatbots provide 24/7 support. Assistant: Can AI help with health care decisions? Yes, AI analyzes medical images and data, predicts medical conditions. User; What's AI's role in cybersecurity? Assistant; AI detects threats, predicts attacks. User's: What is AI's impact on the economy? Assistant's: AI improves productivity, reduces costs, increases efficiency. User’s: What are AI's benefits to society? Assistants: AI\"]\n",
            "Article: User: Hello, how are you? Assistant: I'm good, thank you! How about you? User: What's the difference between deep learning and machine learning? assistant: Deep learning uses neural networks. User: How does AI assist customer service? AI-powered chatbots provide 24/7 support. Assistant: Can AI help with health care decisions? Yes, AI analyzes medical images and data, predicts medical conditions. User; What's AI's role in cybersecurity? Assistant; AI detects threats, predicts attacks. User's: What is AI's impact on the economy? Assistant's: AI improves productivity, reduces costs, increases efficiency. User’s: What are AI's benefits to society? Assistants: AI helps improve productivity, reduce costs, increase efficiency.User's: How is AI different from other technologies?Assistant: AI is more intelligent, faster, and more accurate. User' s: What does AI do for you?Assistant's: Yes, it helps you with tasks requiring human intelligence. User Can AI produce music? Assistant, yes, it does. User can AI create art? assistant, yes. User? What is NLP? Assistant? NLP analyzes and generates human language.User: How's the future of autonomous vehicles? Assistant.: Autonomous vehicles improve safety and efficiency. user: How can AI understand emotions? Assistant:: AI recognizes emotions through affective computing. User:: How does facial recognition work? Assistant:- Facial\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ELqZz2V532QS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}